{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetAttributes:\n",
    "    def __init__(self, n_neurons = 100, \n",
    "                 learning_rate = 0.003, \n",
    "                 num_layers = 1,\n",
    "                 rnn_type = 2,\n",
    "                 n_repeats = 2):\n",
    "        self.n_neurons = n_neurons;\n",
    "        self.learning_rate = learning_rate;\n",
    "        self.num_layers = num_layers;\n",
    "        self.rnn_type = rnn_type;\n",
    "        self.n_repeats = n_repeats\n",
    "        self.n_steps = None\n",
    "        self.n_inputs = None\n",
    "        self.n_outputs = 1\n",
    "        \n",
    "    def set_input_dimension(self, n_steps, n_inputs):\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetStates:\n",
    "    def __init__(self):\n",
    "        self.prediction_states = None\n",
    "        self.training_states = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulLstmModel:\n",
    "    def __init__(self,\n",
    "                net_attributes=None):\n",
    "        if net_attributes != None:\n",
    "            self.net_attributes = net_attributes\n",
    "        else:\n",
    "            self.net_attributes = NetAttributes(n_neurons,\n",
    "                                       learning_rate,\n",
    "                                       num_layers,\n",
    "                                       rnn_type,\n",
    "                                       n_repeats)\n",
    "        self.net_states = NetStates()\n",
    "        self.model_initialized = False\n",
    "    \n",
    "    def __del__(self):\n",
    "        if self.sess != None:\n",
    "            self.sess.close()\n",
    "    \n",
    "    def get_batch(self, seq_index, data_train_input, data_train_output):\n",
    "        X_batch = data_train_input[seq_index:seq_index+1]\n",
    "        y_batch = data_train_output[seq_index:seq_index+1]\n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    \n",
    "    def initialize_layers(self):\n",
    "        layers = None\n",
    "        net_attributes = self.net_attributes\n",
    "        if net_attributes.rnn_type == 0:\n",
    "            layers = [tf.nn.rnn_cell.BasicLSTMCell(net_attributes.n_neurons) \n",
    "              for _ in range(net_attributes.num_layers)]\n",
    "        elif net_attributes.rnn_type == 1:\n",
    "            layers = [tf.nn.rnn_cell.LSTMCell(net_attributes.n_neurons, use_peepholes=False) \n",
    "              for _ in range(net_attributes.num_layers)]\n",
    "        elif net_attributes.rnn_type == 2:\n",
    "            layers = [tf.nn.rnn_cell.LSTMCell(net_attributes.n_neurons, use_peepholes=True) \n",
    "              for _ in range(net_attributes.num_layers)]\n",
    "        else:\n",
    "            print(\"WRONG\")\n",
    "        return layers\n",
    "    \n",
    "    def reset_graph(self, seed=42):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def create_model(self):\n",
    "        net_attributes = self.net_attributes\n",
    "        self.X = tf.placeholder(tf.float32, [None, net_attributes.n_steps, net_attributes.n_inputs])\n",
    "        self.y = tf.placeholder(tf.float32, [None, net_attributes.n_steps, net_attributes.n_outputs])\n",
    "        layers = self.initialize_layers()\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        self.init_state = tf.placeholder(tf.float32, [net_attributes.num_layers, 2, 1, net_attributes.n_neurons])\n",
    "        \n",
    "        state_per_layer_list = tf.unstack(self.init_state, axis=0)\n",
    "        rnn_tuple_state = tuple(\n",
    "            [tf.nn.rnn_cell.LSTMStateTuple(state_per_layer_list[idx][0], state_per_layer_list[idx][1])\n",
    "             for idx in range(net_attributes.num_layers)]\n",
    "        )\n",
    "        \n",
    "        rnn_outputs, self.new_states = tf.nn.dynamic_rnn(cell, self.X, dtype=tf.float32, \n",
    "                                                    initial_state=rnn_tuple_state)\n",
    "        \n",
    "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, net_attributes.n_neurons])\n",
    "        stacked_outputs = tf.layers.dense(stacked_rnn_outputs, net_attributes.n_outputs)\n",
    "        self.outputs = tf.reshape(stacked_outputs, [-1, net_attributes.n_steps, net_attributes.n_outputs])\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.outputs - self.y))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=net_attributes.learning_rate)\n",
    "        self.training_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.model_initialized = True\n",
    "    \n",
    "    # train the model, input is the training data for one cycle\n",
    "    # input is in the shape: [days, steps, features], the features are \n",
    "    # 1. diff, 2. volume. 3. timesteps.\n",
    "    def fit(self,data_train_input, data_train_output, prediction_period, init_states=None):\n",
    "        net_attributes = self.net_attributes\n",
    "        net_states = self.net_states\n",
    "        n_inputs = data_train_input.shape[2]\n",
    "        n_steps = data_train_input.shape[1]\n",
    "        net_attributes.set_input_dimension(n_steps, n_inputs)\n",
    "        batch_size = 1\n",
    "        days = data_train_input.shape[0]\n",
    "        \n",
    "        self.reset_graph()\n",
    "        self.create_model()\n",
    "        my_loss_train_list = []\n",
    "        sess = tf.Session()\n",
    "        # TODO: load from file.\n",
    "\n",
    "        self.init.run(session=sess)\n",
    "        # batch size is 1\n",
    "        if init_states == None:\n",
    "            init_states = np.zeros((net_attributes.num_layers, 2, 1, net_attributes.n_neurons))\n",
    "\n",
    "        for repeat in range(net_attributes.n_repeats):\n",
    "            rnn_states = copy.deepcopy(init_states)\n",
    "            for seq in range(days):\n",
    "                X_batch, y_batch = self.get_batch(seq, data_train_input, data_train_output)\n",
    "                feed_dict = {\n",
    "                        self.X: X_batch,\n",
    "                        self.y: y_batch,\n",
    "                        self.init_state: rnn_states}\n",
    "                my_op, rnn_states, my_loss_train, my_outputs = sess.run([self.training_op, \n",
    "                          self.new_states, \n",
    "                          self.loss, \n",
    "                          self.outputs], feed_dict=feed_dict)\n",
    "\n",
    "                my_loss_train_list.append(my_loss_train)\n",
    "                # last repeat , remember the sates\n",
    "                if seq+1 == prediction_period and repeat == net_attributes.n_repeats-1:\n",
    "                    # next training loop starts from here\n",
    "                    training_states = copy.deepcopy(rnn_states)\n",
    "                my_loss_train_avg = sum(my_loss_train_list) / len(my_loss_train_list)\n",
    "\n",
    "            print(\"{} repeat={} training finished, training MSE={}\".format(\n",
    "                datetime.datetime.now().time(),\n",
    "                repeat, my_loss_train_avg))\n",
    "        \n",
    "        self.net_states.training_states = training_states\n",
    "        self.net_states.prediction_states = rnn_states\n",
    "        self.sess = sess\n",
    "        return\n",
    "    \n",
    "    def predict(self, data_test_input):\n",
    "        net_attributes = self.net_attributes\n",
    "        net_states = self.net_states\n",
    "        days = data_test_input.shape[0]\n",
    "        \n",
    "        rnn_states = copy.deepcopy(net_states.prediction_states)\n",
    "        #X, y, init_state, init, training_op, new_states, loss, outputs = self.create_model()\n",
    "        sess = self.sess\n",
    "        \n",
    "        my_loss_test_list = []\n",
    "        input_shape = data_test_input.shape\n",
    "        outputs_all_days = np.zeros((input_shape[0], input_shape[1], 1))\n",
    "        for seq in range(days):\n",
    "            feed_dict = {\n",
    "                self.X: data_test_input[seq:seq+1],\n",
    "                self.init_state: rnn_states,\n",
    "            }\n",
    "            rnn_states, my_outputs = sess.run([self.new_states, self.outputs], feed_dict=feed_dict)\n",
    "            print(type(my_outputs))\n",
    "            outputs_all_days[seq] = my_outputs\n",
    "        return outputs_all_days\n",
    "        \n",
    "    \n",
    "    def predict_and_verify(self, data_test_input, data_test_output):\n",
    "        net_attributes = self.net_attributes\n",
    "        net_states = self.net_states\n",
    "        days = data_test_input.shape[0]\n",
    "        rnn_states = copy.deepcopy(net_states.prediction_states)\n",
    "        #X, y, init_state, init, training_op, new_states, loss, outputs = self.create_model()\n",
    "        sess = self.sess\n",
    "        \n",
    "        my_loss_test_list = []\n",
    "        input_shape = data_test_input.shape\n",
    "        outputs_all_days = np.zeros((input_shape[0], input_shape[1], 1))\n",
    "        for seq in range(days):\n",
    "            feed_dict = {\n",
    "                self.X: data_test_input[seq:seq+1],\n",
    "                self.y: data_test_output[seq:seq+1],\n",
    "                self.init_state: rnn_states,\n",
    "            }\n",
    "            rnn_states, my_loss_test, my_outputs = sess.run([self.new_states, \n",
    "                                                             self.loss, \n",
    "                                                             self.outputs], \n",
    "                                                            feed_dict=feed_dict)\n",
    "            print(\"sequence:{} test finished, testing MSE={}\".format(seq, my_loss_test))\n",
    "            outputs_all_days[seq] = my_outputs\n",
    "        return outputs_all_days\n",
    "    \n",
    "    \n",
    "    def get_attributes_filename(self, path):\n",
    "        return path + '/net_attributes.pkl'\n",
    "    \n",
    "    def get_states_filename(self, path, date):\n",
    "        return path + '/net_states.pkl'\n",
    "    \n",
    "    def get_model_filename(self, path, date):\n",
    "        return path + '/tf_session.ckpt'\n",
    "    \n",
    "    def save(self, path, date):\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, self.get_model_filename(path))\n",
    "        with open(self.get_attributes_filename(path), 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(self.net_attributes, f, pickle.HIGHEST_PROTOCOL)\n",
    "        with open(self.get_states_filename(path), 'wb') as f:\n",
    "            pickle.dump(self.net_states, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\"Model saved in path: %s\" % path)\n",
    "        \n",
    "            \n",
    "    def load(self, path, date=None):\n",
    "        # TODO: if date is none, load the latest.\n",
    "        # restore hyper-params\n",
    "        with open(self.get_attributes_filename(path), 'rb') as f:\n",
    "            self.net_attributes = pickle.load(f)\n",
    "\n",
    "        # restore states\n",
    "        with open(self.get_states_filename(path), 'rb') as f:\n",
    "            self.net_states = pickle.load(f)\n",
    "        \n",
    "        # 2. restore graph\n",
    "        if self.model_initialized == False:\n",
    "            self.reset_graph()\n",
    "            self.create_model()\n",
    "        \n",
    "        # 3. restore session\n",
    "        saver = tf.train.Saver()\n",
    "        self.sess = tf.Session()\n",
    "        saver.restore(self.sess, self.get_model_filename(path))\n",
    "        print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManipulator:\n",
    "    def __init__(self, beta, ema, time_format, volume_input, use_centralize_bid, \n",
    "                split_daily_data, n_training_days):\n",
    "        self.beta = 99\n",
    "        self.ema = 20\n",
    "        self.time_format = 1\n",
    "        self.volume_input = 0\n",
    "        self.use_centralize_bid = 0\n",
    "        self.split_daily_data = 0\n",
    "        self.n_training_days = 0\n",
    "\n",
    "    def prep_data(self, input_np_data):\n",
    "        # check if we have days more than training period\n",
    "        assert(input_np_data.shape[0] > self.n_training_days)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "class ValueModel:\n",
    "    mixed_domain = [{'name': 'n_neurons', 'type': 'discrete', 'domain': tuple(range(20,160,20))},\n",
    "          {'name': 'learning_rate', 'type': 'discrete', 'domain': (0.001,0.002,0.003,0.004)},\n",
    "          {'name': 'num_layers', 'type': 'discrete', 'domain': (1,2,3,4)},\n",
    "          {'name': 'rnn_type', 'type': 'discrete', 'domain': (0,1,2)},\n",
    "          {'name': 'learning_period', 'type': 'discrete', 'domain': tuple(range(10,41,10))},\n",
    "          {'name': 'prediction_period', 'type': 'discrete', 'domain': (1,2,3,5,8,13)},\n",
    "          {'name': 'max_repeats', 'type': 'discrete', 'domain': tuple(range(1,52,10))},\n",
    "          {'name': 'beta', 'type': 'discrete', 'domain': (99, 98)},\n",
    "          {'name': 'ema', 'type': 'discrete', 'domain': (10,20)},\n",
    "          {'name': 'time_format', 'type': 'discrete', 'domain': (0,1,2)}, #1 for stepofday, 2 for stepofweek\n",
    "          {'name': 'volume_input', 'type': 'discrete', 'domain': (0,1)},\n",
    "          {'name': 'use_centralized_bid', 'type': 'discrete', 'domain': (0,1)},\n",
    "          {'name': 'split_daily_data', 'type': 'discrete', 'domain': (0,1)},\n",
    "          {'name': 'related_stock', 'type': 'discrete', 'domain': (0)},\n",
    "         ]\n",
    "    \n",
    "    def __init__(self, stock_name, stock_index, n_training_days):\n",
    "        self.stock_name = stock_name\n",
    "        self.stock_index = stock_index\n",
    "        self.n_training_days = n_training_days\n",
    "        self.save_path = \"model_{}_{}\".format(stock_name, n_training_days)\n",
    "        return\n",
    "    \n",
    "    def get_max_steps(self, groups):\n",
    "        max_steps = 0\n",
    "        for index, df in groups:\n",
    "            df_len = len(df)\n",
    "            if df_len > max_steps:\n",
    "                max_steps = df_len\n",
    "        return max_steps\n",
    "    \n",
    "    def prep_data(self, input_csv_path):\n",
    "        Path(self.save_path).mkdir(exist_ok=True)\n",
    "        for ema in (10, 20):\n",
    "            for beta in (99, 98):\n",
    "                npy_filename = self.save_path + \"/ema{}_beta{}.npy\".format(ema, beta)\n",
    "                config = Path(npy_filename)\n",
    "                if config.is_file():\n",
    "                    print(\"file: {} exists, ignore...\".format(npy_filename))\n",
    "                \n",
    "                input_filename = input_csv_path + \"/data-prep-ema{}-beta{}.csv\".format(ema, beta)\n",
    "                if not Path(input_filename).is_file():\n",
    "                    print(\"cannot find file: {}, aborting...\".format(input_filename))\n",
    "                    return\n",
    "                \n",
    "                print(\"pre-processing {}\".format(input_filename))\n",
    "                data = pd.read_csv(input_filename, parse_dates=[\"timestamp\"])\n",
    "                groups = data.set_index('timestamp').groupby(lambda x: x.date())\n",
    "                n_steps = self.get_max_steps(groups)\n",
    "                np_data = np.zeros((len(groups), n_steps, 5))\n",
    "                index = self.stock_index\n",
    "                column_list = ['diff_ema_{}_{}'.format(ema, index), \n",
    "                              'volume_{}'.format(index),\n",
    "                              'value_ema_{}_beta_{}_{}'.format(ema, beta, index),\n",
    "                              'step_of_day',\n",
    "                              'step_of_week']\n",
    "                i = 0\n",
    "                for index, df in groups:\n",
    "                    np_data[i] = df[column_list]\n",
    "                    i+=1\n",
    "                np.save(npy_filename, np_data)\n",
    "    \n",
    "    def get_data_prep_desc_filename(self, path):\n",
    "        return path + '/data_prep_desc.pkl'\n",
    "    \n",
    "    def fit(self, input_csv_path, max_iter=300):\n",
    "        self.prep_data(input_csv_path)\n",
    "        opt_handler = GPyOpt.methods.BayesianOptimization(f=self.opt_func,  # Objective function       \n",
    "                                     domain=self.mixed_domain,          # Box-constraints of the problem\n",
    "                                     initial_design_numdata = 30,   # Number data initial design\n",
    "                                     acquisition_type='EI',        # Expected Improvement\n",
    "                                     exact_feval = True)           # True evaluations, no sample noise\n",
    "        opt_handler.run_optimization(max_iter, eps=0)\n",
    "    \n",
    "    def opt_func(self):\n",
    "        answer = np.zeros((X_list.shape[0], 1))\n",
    "        for i in range(len(X_list)):\n",
    "            print(self.get_parameter_str(X_list[i]))\n",
    "            features = X_list[i]\n",
    "            answer[i][0], results_list = self.get_answer(features)\n",
    "            #self.draw_step_profit_graph(self.step_profit_list, \"step_profit_{}\".format(answer[i][0]))\n",
    "            #self.step_profit_list = []\n",
    "            if answer[i][0] < self.min_answer:\n",
    "                print(\"find new opt:{}, {}\".format(answer[i][0], self.get_parameter_str(X_list[i])))\n",
    "                self.min_answer = answer[i][0]\n",
    "            else:\n",
    "                print(\"find result:{}, {}\".format(answer[i][0], self.get_parameter_str(X_list[i])))\n",
    "        return answer\n",
    "    \n",
    "    \n",
    "    def get_result(self, features):\n",
    "        n_neurons = int(features[0])\n",
    "        learning_rate = features[1]\n",
    "        num_layers = int(features[2])\n",
    "        rnn_type = int(features[3])\n",
    "        learning_period = int(features[4])\n",
    "        prediction_period = int(features[5])\n",
    "        max_repeats = int(features[6])\n",
    "        beta = int(features[7])\n",
    "        ema = int(features[8])\n",
    "        time_format = int(features[9])\n",
    "        volume_input = int(features[10])\n",
    "        use_centralized_bid = int(features[11])\n",
    "        split_daily_data = int(features[12])\n",
    "        \n",
    "        data_manipulator = DataManipulator(beta, ema, \n",
    "                                           time_format, \n",
    "                                           volume_input, \n",
    "                                           use_centralize_bid, \n",
    "                                           split_daily_data, \n",
    "                                           self.n_training_days)\n",
    "        data_training_input, data_training_output = data_manipulator.prep_data(self.save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-processing ./data-prep-ema10-beta99.csv\n",
      "pre-processing ./data-prep-ema10-beta98.csv\n",
      "pre-processing ./data-prep-ema20-beta99.csv\n",
      "pre-processing ./data-prep-ema20-beta98.csv\n"
     ]
    }
   ],
   "source": [
    "value_model = ValueModel('Nordea', 5, 60)\n",
    "value_model.fit('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model = StatefulLstmModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"np_ema{}_beta{}.npz\".format(20, 99)\n",
    "data_all = np.load(file_name)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 504, 1)\n",
      "(65, 504, 1)\n",
      "[0.05327712]\n",
      "06:07:37.764473 repeat=0 training finished, training MSE=0.05920288419074495\n",
      "06:07:40.034253 repeat=1 training finished, training MSE=0.030061603643116542\n"
     ]
    }
   ],
   "source": [
    "def transform(data_all, n_inputs, n_outputs):\n",
    "    orig_shape = data_all.shape\n",
    "    data_train_reshape = data_all.reshape((orig_shape[0] * orig_shape[1], orig_shape[2]))\n",
    "\n",
    "    scaler_input = preprocessing.MinMaxScaler().fit(data_train_reshape[:,:n_inputs])\n",
    "    data_train_input_scaled = scaler_input.transform(data_train_reshape[:,:n_inputs])\n",
    "\n",
    "    # the invalid step, we change it to zero!\n",
    "    data_train_input_scaled[~np.any(data_train_reshape, axis=1)] = 0\n",
    "    data_train_input = data_train_input_scaled.reshape(orig_shape[0], orig_shape[1], n_inputs)\n",
    "\n",
    "    scaler_output = preprocessing.MinMaxScaler().fit(data_train_reshape[:,-n_outputs:])\n",
    "    data_train_output_scaled = scaler_output.transform(data_train_reshape[:,-n_outputs:])\n",
    "    # the invalid step, we change it to zero!\n",
    "    data_train_output_scaled[~np.any(data_train_reshape, axis=1)] = 0\n",
    "    data_train_output = data_train_output_scaled.reshape(orig_shape[0], orig_shape[1], n_outputs)\n",
    "    return data_train_input, data_train_output, scaler_output\n",
    "\n",
    "stock_index = 5\n",
    "input_column_list = [30+stock_index]\n",
    "output_column_list = [60+stock_index]\n",
    "all_data = data_all[:,7:-5,input_column_list+output_column_list]\n",
    "data_train_input, data_train_output, scaler_output = transform(all_data, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "print(data_train_input.shape)\n",
    "print(data_train_output.shape)\n",
    "print(scaler_output.data_range_)\n",
    "\n",
    "# TODO: do the scaling outside here!\n",
    "model.fit(data_train_input[:30,:,:],data_train_output[:30,:,:],2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[0.00023405 0.00021665]\n",
      "sequence:0 test finished, testing MSE=0.00023405496904160827\n",
      "sequence:1 test finished, testing MSE=0.00021665381791535765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.85862219],\n",
       "        [0.86148429],\n",
       "        [0.86303729],\n",
       "        ...,\n",
       "        [0.84564275],\n",
       "        [0.84523499],\n",
       "        [0.84493393]],\n",
       "\n",
       "       [[0.84995419],\n",
       "        [0.85443217],\n",
       "        [0.85830563],\n",
       "        ...,\n",
       "        [0.84940988],\n",
       "        [0.8482694 ],\n",
       "        [0.84642345]]])"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.predict(data_train_input[30:32,:,:])\n",
    "y = data_train_output[30:32,:,:]\n",
    "print(np.mean(np.square(outputs-y), axis=(1,2)))\n",
    "model.predict_and_verify(data_train_input[30:32,:,:],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: save\n"
     ]
    }
   ],
   "source": [
    "model.save('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/tf_session.ckpt\n",
      "Model restored.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.85862219],\n",
       "        [0.86148429],\n",
       "        [0.86303729],\n",
       "        ...,\n",
       "        [0.84564275],\n",
       "        [0.84523499],\n",
       "        [0.84493393]],\n",
       "\n",
       "       [[0.84995419],\n",
       "        [0.85443217],\n",
       "        [0.85830563],\n",
       "        ...,\n",
       "        [0.84940988],\n",
       "        [0.8482694 ],\n",
       "        [0.84642345]]])"
      ]
     },
     "execution_count": 677,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load('save')\n",
    "model.predict(data_train_input[30:32,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from save/tf_session.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "new_m = StatefulLstmModel()\n",
    "new_m.load('save')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[0.00023405 0.00021665]\n"
     ]
    }
   ],
   "source": [
    "outputs = new_m.predict(data_train_input[30:32,:,:])\n",
    "y = data_train_output[30:32,:,:]\n",
    "print(np.mean(np.square(outputs-y), axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
